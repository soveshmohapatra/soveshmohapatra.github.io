---
layout: null
permalink: /projects/kans-theory/
---
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deconstructing KANs: Part 1 - Mathematics | Sovesh Mohapatra</title>
    <!-- Google Fonts for Modern Typography -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <!-- Font Awesome for Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <!-- Main Stylesheet (Re-using v2 styles for glassmorphism and theme) -->
    <link rel="stylesheet" href="{{ site.baseurl }}/assets/v2/css/styles.css">

    <!-- Custom Style for Reading Layout -->
    <style>
        .article-container {
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        .article-header {
            margin-bottom: 3rem;
            text-align: center;
        }

        .back-link {
            display: inline-block;
            margin-bottom: 2rem;
            color: var(--accent-light);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.2s ease;
        }

        .back-link:hover {
            color: var(--accent-hover);
        }

        .article-title {
            font-size: 2.5rem;
            line-height: 1.2;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .article-subtitle {
            font-size: 1.25rem;
            color: var(--text-muted);
            font-weight: 400;
            margin-bottom: 1.5rem;
        }

        .article-meta {
            font-size: 0.9rem;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .article-content {
            font-size: 1.1rem;
            line-height: 1.8;
            color: var(--text-color);
        }

        .article-content h2 {
            margin-top: 3rem;
            margin-bottom: 1rem;
            font-size: 1.8rem;
            color: var(--text-color);
            position: relative;
            padding-bottom: 0.5rem;
        }

        .article-content h2::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 50px;
            height: 3px;
            background: var(--accent-gradient);
            border-radius: 2px;
        }

        .article-content h3 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-size: 1.4rem;
            color: var(--text-color);
        }

        .article-content p {
            margin-bottom: 1.5rem;
        }

        .article-content ul,
        .article-content ol {
            margin-bottom: 1.5rem;
            padding-left: 2rem;
        }

        .article-content li {
            margin-bottom: 0.5rem;
        }

        .article-content strong {
            font-weight: 600;
            color: var(--accent-light);
        }

        .math-block {
            overflow-x: auto;
            padding: 1rem 0;
            margin: 1.5rem 0;
            background: rgba(var(--bg-card-rgb), 0.3);
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        .abstract-box {
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 2rem;
            margin-bottom: 3rem;
            box-shadow: 0 4px 6px var(--shadow-color);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
        }

        .abstract-title {
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 1rem;
            font-size: 0.9rem;
            color: var(--accent-light);
        }

        /* Specific dark mode tweaks for mathjax if needed */
        [data-theme="dark"] .MathJax {
            color: var(--text-color) !important;
        }
    </style>

    <!-- MathJax injected to render LaTeX syntax -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                tags: 'ams'
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
        </script>
</head>

<body>

    <!-- Navigation Overlay -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="{{ site.baseurl }}/" class="nav-logo">Sovesh Mohapatra</a>
            <div class="nav-links">
                <a href="{{ site.baseurl }}/">Home</a>
                <a href="{{ site.baseurl }}/about/">About</a>
                <a href="{{ site.baseurl }}/research/">Research</a>
                <a href="{{ site.baseurl }}/projects/" class="active">Projects</a>
            </div>
            <button id="theme-toggle" class="theme-btn" aria-label="Toggle Dark/Light Mode">
                <i class="fas fa-moon"></i>
            </button>
        </div>
    </nav>

    <main class="article-container">

        <a href="{{ site.baseurl }}/projects/kans/" class="back-link">
            <i class="fas fa-arrow-left"></i> Back to KANs Hub
        </a>

        <header class="article-header">
            <h1 class="article-title">Deconstructing Kolmogorov-Arnold Networks (KANs)</h1>
            <h2 class="article-subtitle">Part 1: The Mathematics of Splines on Edges</h2>
            <div class="article-meta">
                <span>By Sovesh Mohapatra</span> &bull; <span>February 2026</span>
            </div>
        </header>

        <article class="article-content">

            <div class="abstract-box">
                <div class="abstract-title">Introduction</div>
                <p>For decades, the standard Multi-Layer Perceptron (MLP) has been the undisputed foundational building
                    block of deep learning paradigms. The architecture is universally taught in week one of any machine
                    learning course: take an input vector, multiply it by a dense weight matrix, sum the results at each
                    node, and pass that scalar through a fixed, non-linear activation function like ReLU, Sigmoid, or
                    Tanh.</p>
                <p>We place the linear operations on the edges (the weights) and the non-linear operations on the nodes
                    (the activation functions). But what if we inverted this paradigm? What if the nodes simply summed
                    the inputs, and the non-linear activation functions lived directly on the <em>edges</em>?</p>
                <p>This is the core premise of Kolmogorov-Arnold Networks (KANs). In this 3-part series, we will
                    completely deconstruct KANs, moving from pure mathematical theory to a functional PyTorch
                    implementation, and finally, a benchmark comparing them against traditional MLPs.</p>
                <p>In Part 1, we will explore the foundational mathematics that makes KANs possible: The
                    Kolmogorov-Arnold Representation Theorem and the mechanics of B-Splines.</p>
            </div>

            <h2>1. The Kolmogorov-Arnold Representation Theorem</h2>
            <p>The theoretical foundation of KANs rests on a mathematical theorem proven by Vladimir Arnold and Andrey
                Kolmogorov in 1957. The theorem states, rather astonishingly, that any multivariate continuous function
                can be represented as a finite composition of continuous functions of a single variable and the
                operation of addition.</p>

            <p>Mathematically, for a continuous function $f : [0,1]^n \to \mathbb{R}$, the theorem is expressed as:</p>

            <div class="math-block">
                $$ f(x_1, \dots, x_n) = \sum_{q=0}^{2n} \Phi_q \left( \sum_{p=1}^{n} \phi_{q,p}(x_p) \right) $$
            </div>

            <p>Where:</p>
            <ul>
                <li>$\phi_{q,p} : [0,1] \to \mathbb{R}$ are inner functions (mapping 1D to 1D).</li>
                <li>$\Phi_q : \mathbb{R} \to \mathbb{R}$ are outer functions (also mapping 1D to 1D).</li>
                <li>We sum over the inputs $p=1$ to $n$, and then sum the outer compositions $q=0$ to $2n$.</li>
            </ul>

            <h3>Implications for Neural Architecture</h3>
            <p>If we view the universal approximation capabilities of MLPs, they rely on depth and width (a large number
                of nodes) to approximate complex functions, using fixed non-linearities. However, the Kolmogorov-Arnold
                theorem suggests that we do not need complex multivariate mappings at all. We only need univariate (1D)
                functions and summation.</p>

            <p>In a KAN layer architecture, instead of a matrix $W \in \mathbb{R}^{out \times in}$, we have a grid of 1D
                functions $\phi_{i,j}$, where each function connects input $i$ to output $j$.</p>

            <p>The output $y_j$ of a KAN node is simply the sum of these non-linear edge functions applied to the
                respective inputs:</p>
            <div class="math-block">
                $$ y_j = \sum_{i=1}^{n_{in}} \phi_{i,j}(x_i) $$
            </div>

            <p>The network learns the functions themselves, rather than just scalar weights.</p>

            <h2>2. Parameterizing Edge Functions: B-Splines</h2>
            <p>To make KANs practical in deep learning, we need a differentiable, expressive way to parameterize these
                1D edge functions $\phi(x)$. We cannot simply learn arbitrary continuous functions without a basis.</p>

            <p>The authors of KAN propose using Basis Splines (B-splines). A B-spline is a piecewise polynomial curve
                defined by a set of control points and a knot vector. This provides localized control: adjusting one
                parameter of the spline only affects a local region of the function, preventing catastrophic forgetting
                and enabling highly stable optimization.</p>

            <p>For an edge function $\phi(x)$, it is decomposed into a residual base activation (like SiLU) and a
                parameterized spline:</p>

            <div class="math-block">
                $$ \phi(x) = w_b \cdot \text{SiLU}(x) + w_s \cdot \text{Spline}(x) $$
            </div>

            <p>The spline itself is a linear combination of B-spline basis functions $B_i(x)$:</p>
            <div class="math-block">
                $$ \text{Spline}(x) = \sum_{i=1}^{c} c_i B_i(x) $$
            </div>

            <p>Here, $c_i$ are the learnable coefficients (the "weights" of the network), and $B_i(x)$ are the fixed
                polynomial basis functions evaluated at $x$.</p>

            <h2>3. Why does this matter?</h2>
            <p>By pushing the non-linearity to the edges and using splines, KANs offer several profound advantages over
                MLPs:</p>

            <ul>
                <li><strong>Interpretability:</strong> Because the edge functions are 1D splines, we can literally plot
                    them. If the network learns a $\sin(x)$ mapping or an $x^2$ mapping, we can visualize the curve
                    directly on the edge. Try doing that with a 10,000x10,000 weight matrix!</li>
                <li><strong>Parameter Efficiency in Symbolic Tasks:</strong> For problems in physics and mathematics,
                    KANs can often achieve higher accuracy than MLPs using orders of magnitude fewer parameters, because
                    they actively learn the underlying symbolic function shape.</li>
                <li><strong>Grid Extension:</strong> We can arbitrarily increase the resolution of the B-spline grids
                    <em>after</em> training without retraining from scratch, allowing for "fine-grained" scaling.</li>
            </ul>

            <h2>Next Steps: Building it in PyTorch</h2>
            <p>The math is beautiful, but the true test is translating formulas into tensor operations. In Part 2 of
                this series, we will drop the theory and open up an IDE. We will construct the B-spline basis
                evaluations and build a 1D KAN layer in pure PyTorch in under 100 lines of code.</p>

        </article>
    </main>

    <!-- Scripts -->
    <script src="{{ site.baseurl }}/assets/v2/js/main.js"></script>
</body>

</html>